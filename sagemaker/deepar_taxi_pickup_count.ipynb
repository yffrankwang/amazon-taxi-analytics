{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting driving taxi pickup count by location with the Amazon SageMaker DeepAR algorithm\n",
    "_**Using the Amazon SageMaker DeepAR algorithm to predict taxi pickup count by location **_\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "1. [Evaluate](#Evaluate)\n",
    "1. [Extensions](#Extensions)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "This article is based on https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/deepar_chicago_traffic_violations/deepar_chicago_traffic_violations.ipynb .\n",
    "\n",
    "#\n",
    "This notebook demonstrates time series forecasting using the Amazon SageMaker DeepAR algorithm by analyzing [New York City Taxi and Limousine Commission (TLC) Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/). \n",
    "\n",
    "The dataset contains pickup/dropoff locations and timestamp. Amazon SageMaker’s DeepAR algorithm can be used to train a model to predict future pickup count and location using the Amazon SageMaker’s [DeepAR algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook was created and tested on an ml.m4.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the notebook instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace `sagemaker.get_execution_role()` with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "#bucket = sess.default_bucket()\n",
    "bucket = \"tlc-stack-artifactbucket-wwus3ytvirwq\"\n",
    "source = \"kinesis-output/20211213/\"\n",
    "prefix = \"sagemaker/nyctlc\"\n",
    "datafile = \"source.csv\"\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import Python libraries like matplotlib, pandas and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os, sys\n",
    "import json, datetime\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset contains 10 minutes time window for timestamp, location, pickup count. If we want to known the next 10 minutes dataset, we can use Amazon SageMaker’s DeepAR algorithm to train a model to predict it.\n",
    "\n",
    "The dataset contains several columns, we use the timestamp, location(geohash), pickup cout for the forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def parse_datetime(ss):\n",
    "\treturn [datetime.datetime.strptime(x, DATE_FORMAT) for x in ss]\n",
    "\n",
    "def pd_read():\n",
    "\t# read the input file, and display sample rows/columns\n",
    "\tpd.set_option(\"display.max_columns\", 500)\n",
    "\tpd.set_option(\"display.max_rows\", 50)\n",
    "\tdf = pd.read_csv(datafile, header=None, \n",
    "\t\tnames=[\"TIMESTAMP\", \"GEOHASH\", \"PICKUP_COUNT\", \"LOCATION\"],\n",
    "\t\tparse_dates=[\"TIMESTAMP\"], date_parser=parse_datetime)\n",
    "\n",
    "\t# print first 10 lines to look at part of the dataset\n",
    "\tprint(\"------------------------------------------------------\")\n",
    "\tprint(df[0:10])\n",
    "\n",
    "\tprint(\"----SORT by TIMESTAMP, GEOHASH------------------------\")\n",
    "\tdf = df.sort_values([\"TIMESTAMP\", \"GEOHASH\"])\n",
    "\tprint(df[0:10])\n",
    "\n",
    "\treturn df\n",
    "\n",
    "df = pd_read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the timestamp from string format to date format, determine the range of datetime, and look at how many unique location we have in our dataset.\n",
    "\n",
    "As described in [Amazon SageMaker DeepAR input/output interface](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar-inputoutput) section, we will convert the data into array, and use 0 for the pickup count when data for a location is not available. Using the Matplotlib library we display each location as a timeseries to visualize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_geohash():\n",
    "\tunique_geohash = df.GEOHASH.unique()\n",
    "\tnumber_of_geohash = len(unique_geohash)\n",
    "\n",
    "\tprint(\"------------------------------------------------------\")\n",
    "\tprint(\"Unique GeoHash {}\".format(number_of_geohash))\n",
    "\tprint(\n",
    "\t\t\"Minimum pickup date is {}, maximum pickup date is {}\".format(\n",
    "\t\t\tdf.TIMESTAMP.min(), df.TIMESTAMP.max()\n",
    "\t\t)\n",
    "\t)\n",
    "\treturn unique_geohash\n",
    "\n",
    "unique_geohash = make_unique_geohash()\n",
    "\n",
    "\n",
    "def make_pickup_list(freq):\n",
    "\tpickup_list = []\n",
    "\tidx = pd.date_range(df.TIMESTAMP.min(), df.TIMESTAMP.max(), freq=freq)\n",
    "\tfor key in unique_geohash:\n",
    "\t\ttemp_df = df[[\"TIMESTAMP\", \"PICKUP_COUNT\"]][df.GEOHASH == key]\n",
    "\t\ttemp_df.set_index([\"TIMESTAMP\"], inplace=True)\n",
    "\t\ttemp_df.index = pd.DatetimeIndex(temp_df.index)\n",
    "\t\ttemp_df = temp_df.reindex(idx, fill_value=0)\n",
    "\t\tpickup_list.append(temp_df[\"PICKUP_COUNT\"])\n",
    "\n",
    "\t# print first 10 items\n",
    "\tprint(\"----PICKUP LIST---------------------------------------\")\n",
    "\tprint(pickup_list[0:10])\n",
    "\treturn pickup_list\n",
    "\n",
    "pickup_list = make_pickup_list(\"10min\")\n",
    "\n",
    "# plot\n",
    "def plot():\n",
    "\tplt.figure(figsize=(12, 6), dpi=100, facecolor=\"w\")\n",
    "\tfor key, geohash in enumerate(unique_geohash):\n",
    "\t\tplt.plot(pickup_list[key], label=geohash)\n",
    "\n",
    "\tplt.ylabel(\"PickupCount\")\n",
    "\tplt.xlabel(\"DateTime\")\n",
    "\tplt.title(\"New York City TLC Pickup Count\")\n",
    "\tplt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.05), shadow=False, ncol=4)\n",
    "\tplt.show()\n",
    "\n",
    "plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define prediction length as 144 (1 day = 6 * 24), and split the data with last 1 day of data as test data. We use rest of the data for training of the model. We can use the last 30 days of data to evaluate the accuracy of our trained model. We write the training and test data files in JSON format in the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1day = 10min * 6 * 24\n",
    "prediction_length = 6 * 24\n",
    "\n",
    "# Split the data for training and test\n",
    "def make_train_list():\n",
    "\ttraining = []\n",
    "\tfor i in pickup_list:\n",
    "\t\ttraining.append((i[:-prediction_length]))\n",
    "\n",
    "\treturn training\n",
    "\n",
    "pickup_list_training = make_train_list()\n",
    "\n",
    "\n",
    "\n",
    "def format_datetime(d):\n",
    "\treturn datetime.datetime.strftime(d, DATE_FORMAT)\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "\tobj = {\n",
    "\t\t\"start\": format_datetime(ts.index[0]), \n",
    "\t\t\"target\": list(ts)\n",
    "\t}\n",
    "\tif cat:\n",
    "\t\tobj[\"cat\"] = cat\n",
    "\treturn obj\n",
    "\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "\treturn json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "\n",
    "encoding = \"utf-8\"\n",
    "def save_train_data():\n",
    "\tfile_name = \"train.json\"\n",
    "\ttrain_data_path = \"{}/train/{}\".format(prefix, file_name)\n",
    "\n",
    "\tprint(\"------------------------------------------------------\")\n",
    "\tprint(\"saving %s\" % file_name)\n",
    "\twith open(file_name, \"wb\") as fp:\n",
    "\t\tfor ts in pickup_list_training:\n",
    "\t\t\tfp.write(series_to_jsonline(ts).encode(encoding))\n",
    "\t\t\tfp.write(\"\\n\".encode(encoding))\n",
    "\n",
    "\tprint(\"uploading %s => %s\" % (file_name, train_data_path))\n",
    "\ts3r.Object(bucket, train_data_path).upload_file(file_name)\n",
    "\n",
    "def save_test_data():\n",
    "\tfile_name = \"test.json\"\n",
    "\ttest_data_path = \"{}/test/{}\".format(prefix, file_name)\n",
    "\n",
    "\tprint(\"------------------------------------------------------\")\n",
    "\tprint(\"saving %s\" % file_name)\n",
    "\twith open(file_name, \"wb\") as fp:\n",
    "\t\tfor ts in pickup_list:\n",
    "\t\t\tfp.write(series_to_jsonline(ts).encode(encoding))\n",
    "\t\t\tfp.write(\"\\n\".encode(encoding))\n",
    "\n",
    "\tprint(\"uploading %s => %s\" % (file_name, test_data_path))\n",
    "\ts3r.Object(bucket, test_data_path).upload_file(file_name)\n",
    "\n",
    "save_train_data()\n",
    "save_test_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train\n",
    "\n",
    "We use [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) to create an [estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) object to kick off training job. The use_spot parameter indicates the use of [managed spot training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html). The training will run at most 30 minutes (1800 seconds). \n",
    "\n",
    "We use the [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) or Hyperparameter optimization for identifying the best values for the [DeepAR hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html). The Automatic Model Tuning job will kick of 10 parallel jobs (set by by max_parallel_jobs) to search the best hyperparameters for this dataset. The jobs will try to minimize the root mean square error on the test dataset using predicted and actual values.\n",
    "\n",
    "You can consider increasing the max_parallel_jobs and max_run and max_wait parameters to allow for finding better hyperparameters, and allow additional tuning of the hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "\tIntegerParameter,\n",
    "\tCategoricalParameter,\n",
    "\tContinuousParameter,\n",
    "\tHyperparameterTuner,\n",
    ")\n",
    "from sagemaker import image_uris\n",
    "\n",
    "\n",
    "container = image_uris.retrieve(region=region, framework=\"forecasting-deepar\")\n",
    "\n",
    "deepar = sagemaker.estimator.Estimator(\n",
    "\tcontainer,\n",
    "\trole,\n",
    "\tinstance_count=1,\n",
    "\tinstance_type=\"ml.m4.xlarge\",\n",
    "\tuse_spot_instances=True,  # use spot instances\n",
    "\tmax_run=1800,  # max training time in seconds\n",
    "\tmax_wait=1800,  # seconds to wait for spot instance\n",
    "\toutput_path=\"s3://{}/{}\".format(bucket, \"estimate\"),\n",
    "\tsagemaker_session=sess,\n",
    ")\n",
    "freq = \"10min\"\n",
    "context_length = 6 * 24\n",
    "\n",
    "deepar.set_hyperparameters(\n",
    "\ttime_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length)\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "\t\"mini_batch_size\": IntegerParameter(100, 400),\n",
    "\t\"epochs\": IntegerParameter(200, 400),\n",
    "\t\"num_cells\": IntegerParameter(30, 100),\n",
    "\t\"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]),\n",
    "\t\"learning_rate\": ContinuousParameter(0.0001, 0.1),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"test:RMSE\"\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "\tdeepar,\n",
    "\tobjective_metric_name,\n",
    "\thyperparameter_ranges,\n",
    "\tmax_jobs=10,\n",
    "\tstrategy=\"Bayesian\",\n",
    "\tobjective_type=\"Minimize\",\n",
    "\tmax_parallel_jobs=10,\n",
    "\tearly_stopping_type=\"Auto\",\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "\ts3_data=\"s3://{}/{}/train/\".format(bucket, prefix), content_type=\"json\"\n",
    ")\n",
    "s3_input_test = sagemaker.inputs.TrainingInput(\n",
    "\ts3_data=\"s3://{}/{}/test/\".format(bucket, prefix), content_type=\"json\"\n",
    ")\n",
    "\n",
    "tuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "We use the [HyperParameterTuner](https://sagemaker.readthedocs.io/en/stable/tuner.html) to host the best model using a single ml.m4.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "best_tuning_job_name = tuner.best_training_job()\n",
    "endpoint = tuner.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tendpoint_name=best_tuning_job_name,\n",
    "\tinstance_type=\"ml.m4.xlarge\",\n",
    "\tserializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "\tdeserializer=JSONDeserializer(),\n",
    "\twait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "To evaluate the model, we define a DeepARPredictor class. This class extends the [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) class. Implementing encode and decode functions helps us make requests using `pandas.Series` objects rather than raw JSON strings.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper().__init__(\n",
    "\t\t\t*args, serializer=IdentitySerializer(content_type=\"application/json\"), **kwargs\n",
    "\t\t)\n",
    "\n",
    "\tdef set_prediction_parameters(self, freq, prediction_length):\n",
    "\t\t\"\"\"Set the time frequency and prediction length parameters. This method **must** be\n",
    "\t\tcalled before being able to use `predict`.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\tfreq -- string indicating the time frequency\n",
    "\t\tprediction_length -- integer, number of predicted time points\n",
    "\n",
    "\t\tReturn value: none.\n",
    "\t\t\"\"\"\n",
    "\t\tself.freq = freq\n",
    "\t\tself.prediction_length = prediction_length\n",
    "\n",
    "\tdef predict(\n",
    "\t\tself, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]\n",
    "\t):\n",
    "\t\t\"\"\"Requests the prediction of for the time series listed in `ts`, each with the\n",
    "\t\t(optional) corresponding category listed in `cat`.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\tts -- list of `pandas.Series` objects, the time series to predict\n",
    "\t\tcat -- list of integers (default: None)\n",
    "\t\tencoding -- string, encoding to use for the request (default: 'utf-8')\n",
    "\t\tnum_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "\t\tquantiles -- list of strings specifying the quantiles to compute (default: ['0.1', '0.5', '0.9'])\n",
    "\n",
    "\t\tReturn value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "\t\t\"\"\"\n",
    "\t\tprediction_times = [x.index[-1] + x.index.freq for x in ts]\n",
    "\t\treq = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "\t\tres = super(DeepARPredictor, self).predict(req)\n",
    "\t\treturn self.__decode_response(res, prediction_times, encoding)\n",
    "\n",
    "\tdef __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "\t\tinstances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "\t\tconfiguration = {\n",
    "\t\t\t\"num_samples\": num_samples,\n",
    "\t\t\t\"output_types\": [\"quantiles\"],\n",
    "\t\t\t\"quantiles\": quantiles,\n",
    "\t\t}\n",
    "\t\thttp_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "\t\treturn json.dumps(http_request_data).encode(encoding)\n",
    "\n",
    "\tdef __decode_response(self, response, prediction_times, encoding):\n",
    "\t\tresponse_data = json.loads(response.decode(encoding))\n",
    "\t\tlist_of_df = []\n",
    "\t\tfor k in range(len(prediction_times)):\n",
    "\t\t\tprediction_index = pd.date_range(\n",
    "\t\t\t\tstart=prediction_times[k], freq=self.freq, periods=self.prediction_length\n",
    "\t\t\t)\n",
    "\t\t\tlist_of_df.append(\n",
    "\t\t\t\tpd.DataFrame(\n",
    "\t\t\t\t\tdata=response_data[\"predictions\"][k][\"quantiles\"], index=prediction_index\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\treturn list_of_df\n",
    "\n",
    "\n",
    "predictor = DeepARPredictor(endpoint_name=endpoint.endpoint_name, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the previously created `predictor` object. We will predict only the first few time series, and compare the results with the actual data we kept in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_prediction_parameters(freq, prediction_length)\n",
    "list_of_df = predictor.predict(pickup_list_training[:5])\n",
    "actual_data = pickup_list[:5]\n",
    "for k in range(len(list_of_df)):\n",
    "\tplt.figure(figsize=(12, 6), dpi=75, facecolor=\"w\")\n",
    "\tplt.ylabel(\"Pickup Count\")\n",
    "\tplt.xlabel(\"Date Time\")\n",
    "\tplt.title(\"NYC TLC Pickup Count:\" + unique_geohash[k])\n",
    "\tactual_data[k][-prediction_length - context_length :].plot(label=\"target\")\n",
    "\tp10 = list_of_df[k][\"0.1\"]\n",
    "\tp90 = list_of_df[k][\"0.9\"]\n",
    "\tplt.fill_between(p10.index, p10, p90, color=\"y\", alpha=0.5, label=\"80% confidence interval\")\n",
    "\tlist_of_df[k][\"0.5\"].plot(label=\"prediction median\")\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Clean-up\n",
    "\n",
    "At the end of this exercise, delete the endpoint to avoid accumulating charges in your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
